# Laboratorio-6
## Samuel Parra yMiguel Caro
## README Taller – Hilos, Sincronización y Docker: 
Este repositorio contiene la implementación de tres puntos de taller orientados a hilos, sincronización y contenedores Docker: (1) análisis de sentimientos sobre reseñas usando hilos y Streamlit, (2) juego 2D tipo Mario Bros usando hilos, mutex y semáforos, y (3) detector de gestos de mano usando MediaPipe, hilos, mutex, semáforos y Docker. Requisitos generales: sistema operativo Ubuntu 22.04 o superior (en el desarrollo se usó Ubuntu 24.10), Git, Docker y servicio Docker activo, Python 3.12 en el host (para puntos 1 y 2), conexión a Internet para instalar dependencias. Instalación base sugerida: ejecutar en terminal ```sudo apt update``` seguido de ```sudo apt install -y git docker.io python3 python3-venv python3-pip``` y luego ```sudo systemctl enable docker``` y ```sudo systemctl start docker```; opcionalmente agregar el usuario al grupo docker con ```sudo usermod -aG docker $USER``` (cerrar sesión y volver a entrar). Para obtener el código del curso se puede clonar el repositorio base con: ```cd ~/Escritorio``` luego ```git clone https://github.com/dialejobv/SistemasDigitales2025II.git``` y ```cd SistemasDigitales2025II```. Dentro de este repositorio se crearon las carpetas de trabajo: Punto1_Sentimientos, Punto2_Juego2D y Punto3_Manos, quedando una estructura tipo: “SistemasDigitales2025II/ Punto1_Sentimientos/ Punto2_Juego2D/ Punto3_Manos/”. 
### Punto 1 – Análisis de sentimientos sobre reseñas: 
* Objetivo: procesar un conjunto de comentarios en paralelo usando hilos de Python, clasificar cada comentario como positivo, negativo o neutro mediante un diccionario simple de palabras, mostrar resultados en una interfaz Streamlit y empaquetar todo en una imagen Docker.
* Estructura de archivos del punto 1 (en carpeta Punto1_Sentimientos): archivos app.py, utils.py, comentarios.txt, requirements.txt y Dockerfile. Contenido lógico de utils.py: se importan hilos y lock con “from threading import Thread, Lock”; se define una lista de palabras positivas PALABRAS_POSITIVAS = [“bueno”, “excelente”, “genial”, “perfecto”, “maravilloso”, “rapido”, “rápido”, “barato”, “funciona”]; se define una lista de palabras negativas PALABRAS_NEGATIVAS = [“malo”, “horrible”, “terrible”, “lento”, “caro”, “defectuoso”, “no funciona”, “pésimo”, “pesimo”]; se crea un lock global “lock_resultados = Lock()”; función “clasificar_sentimiento(texto: str) -> str” que pasa el texto a minúsculas con t = texto.lower(), inicia score = 0, recorre PALABRAS_POSITIVAS y si una palabra está en t suma 1 a score, recorre PALABRAS_NEGATIVAS y si está en t resta 1 a score; si score > 0 retorna “positivo”, si score < 0 retorna “negativo”, en otro caso retorna “neutro”. Luego se define “procesar_comentarios_en_hilos(comentarios, num_hilos=4)” que crea la lista resultados = []; define función interna worker(sublista) donde se crea parciales = [] y por cada comentario c en sublista se calcula sentimiento = clasificar_sentimiento(c), se agrega (c, sentimiento) a parciales; al final de worker se entra en sección crítica con “with lock_resultados:” para “resultados.extend(parciales)”. En la función principal si num_hilos < 1 se fuerza a 1; se calcula tamaño = max(1, len(comentarios) // num_hilos); se crea lista hilos = []; en un ciclo for i in range(0, len(comentarios), tamaño) se toma sub = comentarios[i:i+tamaño], se crea hilo = Thread(target=worker, args=(sub,)), se añade a hilos y se hace hilo.start(); luego se espera a que todos terminen con “for h in hilos: h.join()”; finalmente la función retorna la lista resultados. Contenido lógico de app.py: se importa Streamlit y pandas: “import streamlit as st”, “import pandas as pd” y la función de utilidades con “from utils import procesar_comentarios_en_hilos”; se pone el título con “st.title(“Análisis de sentimientos con hilos”)”; se muestra texto explicativo sobre subir archivo o escribir comentarios; se define “archivo = st.file_uploader(“Archivo de texto (.txt)”, type=[“txt”])”; se define “texto_manual = st.text_area(“Comentarios (un comentario por línea)”)”; se define deslizador de hilos con num_hilos = st.slider(“Número de hilos”, min_value=1, max_value=16, value=4, step=1); cuando se presiona el botón “Analizar sentimientos” con “if st.button(“Analizar sentimientos”):” se genera lista comentarios vacía, si archivo no es None se lee contenido = archivo.read().decode(“utf-8”), se crea comentarios_archivo = [l.strip() for l in contenido.splitlines() if l.strip()], se extiende comentarios con esa lista; si texto_manual tiene contenido se separa por líneas en comentarios_text_area = [l.strip() for l in texto_manual.splitlines() if l.strip()] y también se añaden a comentarios; si comentarios está vacío se muestra “st.warning(“No hay comentarios para analizar.”)”, en caso contrario se muestra la cantidad total con st.write, luego se llama “resultados = procesar_comentarios_en_hilos(comentarios, num_hilos=num_hilos)”; se construye df = pd.DataFrame(resultados, columns=[“Comentario”, “Sentimiento”]); se muestra con st.dataframe(df); se calcula resumen = df[“Sentimiento”].value_counts() y se grafica con st.bar_chart(resumen). Archivo comentarios.txt es un ejemplo de reseñas, por ejemplo: “La pantalla de este portátil es excelente.” “El sonido es muy malo y se escucha distorsionado.” “Funciona bien pero la batería es regular.” (una reseña por línea). Archivo requirements.txt del punto 1 contiene las dependencias: “streamlit” y “pandas”. Dockerfile del punto 1: se basa en la imagen “FROM python:3.11-slim”; define “WORKDIR /app”; copia requirements.txt con “COPY requirements.txt .”; instala dependencias con “RUN pip install –no-cache-dir -r requirements.txt”; copia el resto del código con “COPY . .”; expone el puerto 8501 con “EXPOSE 8501”; y define el comando de inicio de la app con Streamlit: “CMD [“streamlit”, “run”, “app.py”, “–server.port=8501”, “–server.address=0.0.0.0”]”. Para ejecución local del punto 1: en la carpeta Punto1_Sentimientos ejecutar “python3 -m venv .venv”, luego “source .venv/bin/activate”, después “pip install -r requirements.txt” y finalmente “streamlit run app.py”, abriendo en navegador “http://localhost:8501”. Para ejecución del punto 1 en Docker: en Punto1_Sentimientos ejecutar “docker build -t punto1_sentimientos .” y luego “docker run –rm -p 8501:8501 punto1_sentimientos”, accediendo de nuevo en el navegador a “http://localhost:8501”.
### Punto 2 – Juego 2D tipo Mario Bros con hilos: 
* Objetivo: implementar un minijuego 2D tipo plataformas usando Pygame, con un hilo principal para el loop del juego y hilos secundarios para enemigos, generación de monedas y detección de colisiones; usar Lock para proteger estructuras compartidas y un Semaphore para limitar el número de monedas, y empaquetar en Docker.
* Estructura de archivos del punto 2 (carpeta Punto2_Juego2D): main.py, game_objects.py, requirements.txt y Dockerfile. Contenido lógico de game_objects.py: se importa pygame con “import pygame”; se definen colores como tuplas RGB: COLOR_PLAYER = (255, 0, 0) para el jugador, COLOR_ENEMY = (0, 255, 0) para enemigos, COLOR_COIN = (255, 255, 0) para monedas; se define clase Player con init(self, x, y) que crea “self.rect = pygame.Rect(x, y, 40, 40)” y “self.vel_y = 0”; método update(self, keys_pressed) que si keys_pressed[pygame.K_LEFT] decrementa self.rect.x en 5 y si keys_pressed[pygame.K_RIGHT] incrementa self.rect.x en 5; se define clase Enemy con init(self, x, y, direction=1) que crea “self.rect = pygame.Rect(x, y, 40, 40)” y “self.direction = direction”; método update(self) que mueve horizontalmente con “self.rect.x += 2 * self.direction”. Contenido lógico de main.py: se importan módulos pygame, threading, Lock y Semaphore y random: “import pygame”, “import threading”, “from threading import Lock, Semaphore”, “import random”, más las clases y colores: “from game_objects import (Player, Enemy, COLOR_PLAYER, COLOR_ENEMY, COLOR_COIN)”; se inicializa Pygame con pygame.init(); se definen dimensiones WIDTH, HEIGHT = 800, 600; se crea pantalla con “screen = pygame.display.set_mode((WIDTH, HEIGHT))”; se pone título “pygame.display.set_caption(“Punto 2 - Juego 2D con hilos”)”; se crea reloj “clock = pygame.time.Clock()”; se crea jugador “player = Player(100, 500)”; se crea lista de enemigos “enemies = [Enemy(300, 500, 1), Enemy(500, 500, -1)]”; se inicializa lista de coins vacía: coins = []; se crea “lock_objetos = Lock()”; se crea “sem_coins = Semaphore(5)” para limitar el máximo de monedas activas; se define running = True. Se define función hilo_enemigos() que en un while running hace “with lock_objetos:” y recorre enemies para e.update(), y si e.rect.x sale de los límites (menor a 0 o mayor a WIDTH - e.rect.width) se invierte e.direction con “e.direction = -1”; luego duerme 30 ms con “pygame.time.wait(30)”. Se define hilo_monedas() que en while running espera 1000 ms con pygame.time.wait(1000), luego intenta “sem_coins.acquire(blocking=False)” y si tiene éxito entra en “with lock_objetos:” y crea un rectángulo para la moneda: rect = pygame.Rect(random.randint(0, WIDTH - 20), random.randint(300, 540), 20, 20); lo añade a coins con coins.append(rect). Se define hilo_colisiones() que en while running entra en “with lock_objetos:” y recorre una copia de coins con “for c in coins[:]”, si player.rect.colliderect(c) entonces coins.remove(c) y “sem_coins.release()”; duerme 50 ms con pygame.time.wait(50). Se lanzan los hilos con “threading.Thread(target=hilo_enemigos, daemon=True).start()”, “threading.Thread(target=hilo_monedas, daemon=True).start()” y “threading.Thread(target=hilo_colisiones, daemon=True).start()”. Luego el loop principal: while running, se procesan eventos con “for event in pygame.event.get(): if event.type == pygame.QUIT: running = False”; se leen teclas con keys = pygame.key.get_pressed(); se llama player.update(keys); se limpia pantalla con “screen.fill((0, 0, 0))”; se dibuja una plataforma con “pygame.draw.rect(screen, (150, 75, 0), (0, 550, 800, 50))”; en “with lock_objetos:” se dibuja el jugador con pygame.draw.rect(screen, COLOR_PLAYER, player.rect), luego cada enemigo con pygame.draw.rect(screen, COLOR_ENEMY, e.rect), y cada moneda con pygame.draw.rect(screen, COLOR_COIN, c); se actualiza pantalla con pygame.display.flip(); se regula FPS con “clock.tick(60)”; al salir, se llama pygame.quit(). Archivo requirements.txt del punto 2 contiene simplemente “pygame”. Dockerfile del punto 2: usa “FROM python:3.11-slim”; instala dependencias de X11 con “RUN apt-get update && apt-get install -y python3-dev libx11-6 libxext6 libxrender1 libxrandr2 libxi6 && rm -rf /var/lib/apt/lists/”; define “WORKDIR /app”; copia requirements.txt y ejecuta “RUN pip install –no-cache-dir -r requirements.txt”; copia el resto de archivos con “COPY . .”; establece “CMD [“python”, “main.py”]”. Para ejecutar localmente el punto 2: en Punto2_Juego2D usar “python3 -m venv .venv”, luego “source .venv/bin/activate”, “pip install -r requirements.txt” y “python main.py”; el jugador se mueve con flechas izquierda y derecha, y se cierra con la X de la ventana. Para ejecutar en Docker con interfaz gráfica se debe permitir acceso X11 con “xhost +local:docker”, luego en Punto2_Juego2D construir “docker build -t punto2_juego2d .” y ejecutar “docker run –rm -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix punto2_juego2d”.
### Punto 3 – Detector de gestos de mano con MediaPipe: 
* Objetivo: crear una aplicación que capture video en tiempo real, procese los frames en paralelo usando MediaPipe Hands para detectar landmarks de la mano, use dos hilos (captura y procesamiento) sincronizados con Lock y Semaphore, muestre el resultado en una interfaz Streamlit y se ejecute dentro de un contenedor Docker basado en Python 3.11 (para resolver la compatibilidad de mediapipe en Linux), aunque el host use Python 3.12.
* Estructura de archivos del punto 3 (carpeta Punto3_Manos): app.py, hand_detector.py, requirements.txt y Dockerfile. Contenido lógico de hand_detector.py: se importa OpenCV y MediaPipe con “import cv2” y “import mediapipe as mp”; se obtiene mp_hands = mp.solutions.hands y mp_drawing = mp.solutions.drawing_utils; se crea el objeto global hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5); se define la función procesar_frame(frame_bgr) que recibe un frame BGR de OpenCV, convierte a RGB con image_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB), llama results = hands.process(image_rgb), copia el frame con annotated_image = frame_bgr.copy(); si results.multi_hand_landmarks no es None, recorre cada hand_landmarks y dibuja con mp_drawing.draw_landmarks(annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS); devuelve annotated_image y results. Contenido lógico de app.py del punto 3: se importan módulos: “import cv2”, “import time”, “import threading”, “from threading import Lock, Semaphore”, “import streamlit as st”, “from hand_detector import procesar_frame”; se definen variables globales compartidas: frame_original = None, frame_procesado = None, lock_frame = Lock(), sem_nuevo_frame = Semaphore(0), running = True. Se define hilo_captura(): dentro se usa “cap = cv2.VideoCapture(0)” y si no se abre imprime “No se pudo abrir la cámara.” y pone running = False; en un while running se lee ret, frame = cap.read(); si no ret se rompe; se actualiza frame_original dentro de sección crítica “with lock_frame: frame_original = frame”; se intenta sem_nuevo_frame.release() dentro de un try/except ValueError para ignorar desbordes; se espera 0.02 segundos con time.sleep(0.02); al final se libera la cámara con cap.release(). Se define hilo_procesamiento(): en un while running se bloquea con sem_nuevo_frame.acquire(); dentro de “with lock_frame:” si frame_original es None se continúa, en caso contrario se copia frame = frame_original.copy(); fuera de la sección crítica se llama procesado, _ = procesar_frame(frame); luego se asigna frame_procesado = procesado. Como Streamlit reejecuta el script, se usa st.session_state para que los hilos se lancen una única vez: “if “hilos_iniciados” not in st.session_state: st.session_state[“hilos_iniciados”] = True; t1 = threading.Thread(target=hilo_captura, daemon=True); t2 = threading.Thread(target=hilo_procesamiento, daemon=True); t1.start(); t2.start()”. Luego se define la interfaz: título con st.title(“Detector de gestos de mano con MediaPipe, hilos y semáforos”); se explica el uso de hilos y sincronización en st.markdown; se crea un placeholder = st.empty(); en un bucle while True se revisa si frame_procesado no es None, se convierte a RGB con imagen_rgb = cv2.cvtColor(frame_procesado, cv2.COLOR_BGR2RGB) y se muestra con placeholder.image(imagen_rgb, channels=“RGB”); si no hay frame_procesado aún, se muestra “placeholder.write(“Esperando imagen de la cámara…”)”; se espera 0.03 segundos con time.sleep(0.03). Archivo requirements.txt del punto 3 incluye: “streamlit”, “opencv-python” y “mediapipe==0.10.11” (versión compatible con Python 3.11 en Linux). Dockerfile del punto 3: usa “FROM python:3.11-slim”; instala librerías necesarias para OpenCV y video con “RUN apt-get update && apt-get install -y ffmpeg libsm6 libxext6 && rm -rf /var/lib/apt/lists/”; define “WORKDIR /app”; copia requirements.txt y ejecuta “RUN pip install –no-cache-dir -r requirements.txt”; copia todo el código con “COPY . .”; expone puerto 8501 con “EXPOSE 8501”; y ejecuta la app con Streamlit usando “CMD [“streamlit”, “run”, “app.py”, “–server.port=8501”, “–server.address=0.0.0.0”]”. Para construir la imagen del punto 3 se ejecuta en Punto3_Manos: “docker build -t punto3_manos .”. Para ejecutarlo con la cámara se identifica primero el dispositivo con “ls -l /dev/video” (por ejemplo /dev/video0 y /dev/video1) y luego se arranca el contenedor con privilegios y el dispositivo de vídeo mapeado, por ejemplo “sudo docker run –rm -p 8501:8501 –privileged –device=/dev/video0 punto3_manos” o, si es otra cámara, “sudo docker run –rm -p 8501:8501 –privileged –device=/dev/video1 punto3_manos”; finalmente se accede a “http://localhost:8501” en el navegador y se observa el video de la cámara con los landmarks de MediaPipe. Resumen de concurrencia y sincronización: en el punto 1 se crean múltiples hilos que procesan subconjuntos de comentarios y se coordina el acceso a la lista global de resultados con un Lock para evitar condiciones de carrera; en el punto 2 se usa un hilo principal para el juego y hilos secundarios para enemigos, generación de monedas y colisiones, usando Lock para proteger las listas compartidas y un Semaphore para limitar el número máximo de monedas; en el punto 3 se tienen un hilo productor de frames y un hilo consumidor que procesa los frames con MediaPipe, sincronizados mediante Lock para proteger las variables compartidas frame_original y frame_procesado y un Semaphore para notificar la disponibilidad de nuevos frames, además de integrar todo en contenedores Docker con las versiones de Python necesarias para la compatibilidad de librerías.

![WhatsApp Image 2025-11-21 at 6 23 02 PM(2)](https://github.com/user-attachments/assets/96747e97-f968-4bbc-b646-5767e4257f2f)
![WhatsApp Image 2025-11-21 at 6 23 02 PM(1)](https://github.com/user-attachments/assets/2dc672d4-1888-4c8b-a3d5-b1f861e9a892)
![WhatsApp Image 2025-11-21 at 6 23 02 PM](https://github.com/user-attachments/assets/b09653fc-a249-4f1a-8141-0dcf479bb2df)
![WhatsApp Image 2025-11-21 at 6 21 22 PM(5)](https://github.com/user-attachments/assets/e704e9e7-71e2-4395-92e2-9591a8ad5ecf)
![WhatsApp Image 2025-11-21 at 6 21 22 PM(4)](https://github.com/user-attachments/assets/265de454-b503-4d6e-9e72-d4653aedd6fd)
![WhatsApp Image 2025-11-21 at 6 21 22 PM(3)](https://github.com/user-attachments/assets/d9c2f309-4faa-485d-a685-994c22b9ecd2)
![WhatsApp Image 2025-11-21 at 6 21 22 PM(2)](https://github.com/user-attachments/assets/6f4614f4-60d0-4a55-a3b2-08a349cd1453)
![WhatsApp Image 2025-11-21 at 8 43 36 PM(9)](https://github.com/user-attachments/assets/7f4ebab2-79cd-4c1b-8bfa-ad3aa867e312)
![WhatsApp Image 2025-11-21 at 8 43 36 PM(8)](https://github.com/user-attachments/assets/3020078b-d2c1-4e37-a2dd-391e782a571c)
![WhatsApp Image 2025-11-21 at 8 43 36 PM(7)](https://github.com/user-attachments/assets/140fdfbb-2bc0-4a3c-b8df-a8c66be24636)
![WhatsApp Image 2025-11-21 at 8 43 36 PM(6)](https://github.com/user-attachments/assets/555fa7d6-3c84-4519-800c-8d20d3a58c46)
![WhatsApp Image 2025-11-21 at 8 43 36 PM(5)](https://github.com/user-attachments/assets/d5a32fb6-aa8c-41e7-9f6f-bf8c9e6b7a31)
![WhatsApp Image 2025-11-21 at 8 43 36 PM(4)](https://github.com/user-attachments/assets/916c45e2-bee5-4a47-9a9e-3b9dd6861815)
![WhatsApp Image 2025-11-21 at 8 43 36 PM(3)](https://github.com/user-attachments/assets/1605b178-e732-4519-b861-a65baee1fa2f)
![WhatsApp Image 2025-11-21 at 8 43 36 PM(2)](https://github.com/user-attachments/assets/5f6c293b-20f3-48ed-9797-bca38af17093)
![WhatsApp Image 2025-11-21 at 8 43 36 PM(1)](https://github.com/user-attachments/assets/70a0b11c-8808-49d8-a449-f9fd71538987)
![WhatsApp Image 2025-11-21 at 8 43 36 PM](https://github.com/user-attachments/assets/1aa09d5b-3e90-4952-9a92-550cfcd63fae)

![WhatsApp Image 2025-11-21 at 6 21 22 PM(1)](https://github.com/user-attachments/assets/5f7ccde9-bdfb-4b39-9136-fb3f82dc52e7)
![WhatsApp Image 2025-11-21 at 6 21 22 PM](https://github.com/user-attachments/assets/4ca67756-2890-40e9-ab49-ea0fd7196ff5)

![WhatsApp Image 2025-11-21 at 7 26 07 PM](https://github.com/user-attachments/assets/a072337f-bb30-45ff-a34d-c3f40fe743b5)
![WhatsApp Image 2025-11-21 at 7 25 12 PM(7)](https://github.com/user-attachments/assets/228524cf-1faf-4ca0-b231-ab91cedb2b57)
![WhatsApp Image 2025-11-21 at 7 25 12 PM(6)](https://github.com/user-attachments/assets/7218e3a9-3475-4709-a29b-5f131fd58d8c)
![WhatsApp Image 2025-11-21 at 7 25 12 PM(5)](https://github.com/user-attachments/assets/73fa1245-cdd6-4bdb-908b-0b8ed8339a97)
![WhatsApp Image 2025-11-21 at 7 25 12 PM(4)](https://github.com/user-attachments/assets/8ddbc314-72ed-41f7-ab5a-443488ac4114)
![WhatsApp Image 2025-11-21 at 7 25 12 PM(3)](https://github.com/user-attachments/assets/cc6571b5-bb4e-4639-a237-923302073db2)
![WhatsApp Image 2025-11-21 at 7 25 12 PM(2)](https://github.com/user-attachments/assets/9f48530b-78e7-4104-b4d7-f98f1a05dbbe)
![WhatsApp Image 2025-11-21 at 7 25 12 PM(1)](https://github.com/user-attachments/assets/27f5c2ba-9fac-4eea-9afa-b09d8db0598c)
![WhatsApp Image 2025-11-21 at 7 25 12 PM](https://github.com/user-attachments/assets/9a17f796-2335-40c8-884d-ee35a57d8c23)
